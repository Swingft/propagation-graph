import os
import json
import shutil
from multiprocessing import Pool, cpu_count
from pathlib import Path
import re
from collections import defaultdict


SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent
INPUT_ROOT_DIR = PROJECT_ROOT / 'input_label'
OUTPUT_ROOT_DIR = PROJECT_ROOT / 'output_label'
MODEL_SUB_DIRS = ['claude_generated', 'gemini_generated', 'gpt_generated']

SPLIT_INPUT_DIR = PROJECT_ROOT / 'input_label_split'
SPLIT_OUTPUT_DIR = PROJECT_ROOT / 'output_label_split'

CONTEXT_MAP = {
    'methods': ['classes', 'structs', 'enums', 'protocols', 'extensions', 'methods', 'initializers', 'deinitializers',
                'subscripts', 'variables'],
    'properties': ['classes', 'structs', 'enums', 'protocols', 'extensions', 'properties', 'variables', 'enumCases'],
    'variables': ['classes', 'structs', 'enums', 'protocols', 'extensions', 'variables', 'properties', 'methods'],
    'initializers': ['classes', 'structs', 'enums', 'protocols', 'extensions', 'initializers'],
    'deinitializers': ['classes', 'structs', 'enums', 'protocols', 'extensions', 'deinitializers'],
    'subscripts': ['classes', 'structs', 'enums', 'protocols', 'extensions', 'subscripts'],
    'enumCases': ['enums', 'enumCases'],
    'classes': ['classes', 'protocols', 'structs'],
    'structs': ['structs', 'protocols', 'classes'],
    'enums': ['enums', 'protocols', 'enumCases', 'classes'],
    'protocols': ['protocols'],
    'extensions': ['extensions', 'classes', 'structs', 'enums', 'protocols']
}

TARGET_CATEGORIES = list(CONTEXT_MAP.keys())


def setup_directories():
    """ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤."""
    print("ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
    if SPLIT_INPUT_DIR.exists(): shutil.rmtree(SPLIT_INPUT_DIR)
    if SPLIT_OUTPUT_DIR.exists(): shutil.rmtree(SPLIT_OUTPUT_DIR)
    SPLIT_INPUT_DIR.mkdir(exist_ok=True)
    SPLIT_OUTPUT_DIR.mkdir(exist_ok=True)
    print("ë””ë ‰í† ë¦¬ ì¤€ë¹„ ì™„ë£Œ.")


def process_file_pair(task_info):
    """
    í•˜ë‚˜ì˜ (input.json, output.json) ìŒì„ ì¹´í…Œê³ ë¦¬ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    ê³ ì‹ ë¢°ë„ Positive/Negative ìƒ˜í”Œì„ ëª¨ë‘ ìƒì„±í•©ë‹ˆë‹¤.
    """
    input_file_path, output_file_path, model_dir = task_info
    try:
        with open(input_file_path, 'r', encoding='utf-8') as f:
            full_input_data = json.load(f)

        output_data = {}
        if output_file_path.exists():
            with open(output_file_path, 'r', encoding='utf-8') as f:
                output_data = json.load(f)

        mapping_data = full_input_data.get('mapping', {})
        input_data = full_input_data.get('data', {})
        meta_data = input_data.get('meta', {})
        input_decisions = input_data.get('decisions', {})
        base_name = re.sub(r'^(input_|output_)', '', input_file_path.stem)

        for category in TARGET_CATEGORIES:

            required_context_keys = CONTEXT_MAP.get(category)
            if required_context_keys is None:
                continue

            new_decisions = {}
            for key in required_context_keys:
                if key in input_decisions:
                    new_decisions[key] = input_decisions[key]

            if not new_decisions:
                continue

            final_input_structure = {"mapping": mapping_data, "data": {"meta": meta_data, "decisions": new_decisions}}
            input_save_dir = SPLIT_INPUT_DIR / model_dir / category
            input_save_dir.mkdir(parents=True, exist_ok=True)
            input_filename = f"input_{base_name}_{category}.json"

            if category in output_data and output_data.get(category):
                # Positive Sample: ì •ë‹µì´ ìˆìœ¼ë¯€ë¡œ Inputê³¼ Outputì„ ëª¨ë‘ ìƒì„±
                with open(input_save_dir / input_filename, 'w', encoding='utf-8') as f:
                    json.dump(final_input_structure, f, ensure_ascii=False, indent=2)

                split_output_data = {category: output_data[category]}
                output_save_dir = SPLIT_OUTPUT_DIR / model_dir / category
                output_save_dir.mkdir(parents=True, exist_ok=True)
                output_filename = f"output_{base_name}_{category}.json"
                with open(output_save_dir / output_filename, 'w', encoding='utf-8') as f:
                    json.dump(split_output_data, f, ensure_ascii=False, indent=2)
            else:
                # ê³ ì‹ ë¢°ë„ Negative ìƒ˜í”Œë§ ë¡œì§ ì ìš©
                # Input ì›ë³¸ì— í•´ë‹¹ ì¹´í…Œê³ ë¦¬ê°€ ì¡´ì¬í•  ë•Œë§Œ Negative ìƒ˜í”Œë¡œ ê°„ì£¼í•˜ì—¬ ìƒì„±í•©ë‹ˆë‹¤.
                # ì˜ˆë¥¼ ë“¤ì–´, Inputì— 'properties'ê°€ ìˆì—ˆëŠ”ë°ë„ Outputì— ì—†ë‹¤ë©´,
                # ì´ëŠ” "ì²˜ë¦¬í•  propertiesê°€ ìˆì—ˆì§€ë§Œ ëª¨ë‘ ì œì™¸ëŒ€ìƒì´ ì•„ë‹ˆì—ˆë‹¤"ëŠ” ê°•ë ¥í•œ ì¦ê±°ê°€ ë©ë‹ˆë‹¤.
                if category in input_decisions:
                    with open(input_save_dir / input_filename, 'w', encoding='utf-8') as f:
                        json.dump(final_input_structure, f, ensure_ascii=False, indent=2)

        return {"model": model_dir, "status": "SUCCESS", "message": f"'{input_file_path.name}' ì²˜ë¦¬"}

    except Exception as e:
        return {"model": model_dir, "status": "ERROR", "message": f"'{input_file_path.name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"}


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    setup_directories()
    tasks = []
    print("ì²˜ë¦¬í•  Input/Output íŒŒì¼ ìŒì„ ê²€ìƒ‰ ë° ë§¤ì¹­í•©ë‹ˆë‹¤...")

    input_files_map, output_files_map = {}, {}
    for sub_dir in MODEL_SUB_DIRS:
        input_dir = INPUT_ROOT_DIR / sub_dir
        if input_dir.is_dir():
            count = 0
            for filename in os.listdir(input_dir):
                match = re.search(r'input_(.+)\.json', filename)
                if match:
                    key = match.group(1)
                    input_files_map[(key, sub_dir)] = input_dir / filename
                    count += 1
            print(f"   - '{sub_dir}'ì—ì„œ Input íŒŒì¼ {count}ê°œ ë°œê²¬")

        output_dir = OUTPUT_ROOT_DIR / sub_dir
        if output_dir.is_dir():
            for filename in os.listdir(output_dir):
                match = re.search(r'output_(.+)\.json', filename)
                if match:
                    key = match.group(1)
                    output_files_map[(key, sub_dir)] = output_dir / filename

    for (key, model_dir), input_path in input_files_map.items():
        output_path = output_files_map.get((key, model_dir), Path())
        tasks.append((input_path, output_path, model_dir))

    if not tasks:
        print("ì²˜ë¦¬í•  íŒŒì¼ ìŒì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ ì´ë¦„ ê·œì¹™(input_*.json, output_*.json)ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    print(f"\nì´ {len(tasks)}ê°œì˜ íŒŒì¼ ì„¸íŠ¸ì— ëŒ€í•´ ë¶„í•  ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    with Pool(processes=cpu_count()) as pool:
        results = pool.map(process_file_pair, tasks)

    print("\n\n" + "=" * 50)
    print("ğŸ“Š ìµœì¢… ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
    print("=" * 50)

    file_summary = defaultdict(lambda: defaultdict(int))
    error_details = []

    for res in results:
        model = res["model"]
        status = res["status"]

        file_summary[model][status] += 1

        if status == "ERROR":
            error_details.append(f"[{model}] {res['message']}")

    grand_total_files = 0
    for model in sorted(file_summary.keys()):
        stats = file_summary[model]
        total_files = sum(stats.values())
        grand_total_files += total_files

        print(f"\n--- ëª¨ë¸: {model} (ì´ {total_files}ê°œ íŒŒì¼) ---")
        print(f"  - âœ… íŒŒì¼ ì²˜ë¦¬ ì„±ê³µ: {stats.get('SUCCESS', 0)}ê°œ")
        print(f"  - ğŸ”¥ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {stats.get('ERROR', 0)}ê°œ")

    print("\n" + "=" * 50)
    print(f"ğŸ“ˆ ì „ì²´ ì²˜ë¦¬ íŒŒì¼ ìˆ˜: {grand_total_files}ê°œ")
    print("=" * 50)

    if error_details:
        print("\n\n" + "ğŸ”¥ ì˜¤ë¥˜ ìƒì„¸ ë‚´ì—­:")
        print("-" * 40)
        for detail in sorted(error_details):
            print(detail)

    print("\nëª¨ë“  íŒŒì¼ ë¶„í•  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == '__main__':
    main()