import os
import json
from pathlib import Path
from typing import Dict, Any, List, Optional

SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent
BASE_DIR = PROJECT_ROOT / "jsonl"
INPUT_DIR = BASE_DIR / "input_label_split"
OUTPUT_DIR = BASE_DIR / "output_label_split"
FINAL_DATASET_FILE = PROJECT_ROOT / "alpaca_dataset.jsonl"
NO_EXCLUSION_MESSAGE = "There are no identifiers that need to be excluded from obfuscation."


def map_output_files(output_dir: Path) -> Dict[str, Path]:
    """
    output ë””ë ‰í† ë¦¬ë¥¼ ìŠ¤ìº”í•˜ì—¬ 'output_' ì ‘ë‘ì‚¬ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì„ keyë¡œ,
    íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œë¥¼ valueë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    file_map = {}
    if not output_dir.is_dir():
        return file_map

    for file_path in output_dir.rglob("output_*.jsonl"):
        key = file_path.name.replace("output_", "", 1)
        file_map[key] = file_path

    print(f"Found and mapped {len(file_map)} output files.")
    return file_map


def create_alpaca_dataset():
    """
    'input_*.jsonl' íŒŒì¼ í•˜ë‚˜ë¥¼ í†µì§¸ë¡œ 'input'ìœ¼ë¡œ, 'output_*.jsonl' íŒŒì¼ í•˜ë‚˜ë¥¼
    í†µì§¸ë¡œ 'output'ìœ¼ë¡œ í•˜ëŠ” Alpaca ë°ì´í„°ì…‹ ë ˆì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not INPUT_DIR.is_dir():
        print(f"Error: Input directory not found at '{INPUT_DIR}'")
        return

    all_records: List[Dict[str, str]] = []
    main_instruction = None
    no_exclusion_count = 0  # Negative ìƒ˜í”Œ ì¹´ìš´í„°
    files_processed = 0

    print("Mapping all output files first...")
    output_file_map = map_output_files(OUTPUT_DIR)

    print("Starting dataset creation...")

    for input_file_path in INPUT_DIR.rglob("input_*.jsonl"):
        files_processed += 1
        file_mapping = None
        symbol_lines_data = []

        try:
            with open(input_file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        data = json.loads(line)
                        if 'meta' in data:
                            if main_instruction is None and 'prompt_context' in data['meta']:
                                main_instruction = data['meta']['prompt_context']
                                print("Successfully extracted main instruction.")
                            if 'mapping' in data['meta']:
                                file_mapping = data['meta']['mapping']
                        else:
                            symbol_lines_data.append(data)
                    except json.JSONDecodeError:
                        continue
        except Exception as e:
            print(f"Warning: Could not read {input_file_path}. Error: {e}")
            continue

        if not main_instruction or not file_mapping:
            print(f"Warning: Critical meta information missing in {input_file_path.name}. Skipping.")
            continue

        new_input_obj = {
            "mapping": file_mapping,
            "symbols": symbol_lines_data  # 'symbol_data' -> 'symbols' (ë³µìˆ˜í˜•), ë¦¬ìŠ¤íŠ¸ ì „ì²´ë¥¼ í• ë‹¹
        }
        input_string = json.dumps(new_input_obj, ensure_ascii=False)

        output_string = ""
        file_suffix = input_file_path.name.replace("input_", "", 1)

        # Positive Sample ì²˜ë¦¬
        if file_suffix in output_file_map:
            output_file_path = output_file_map[file_suffix]
            try:
                with open(output_file_path, 'r', encoding='utf-8') as f_out:
                    output_lines_data = [json.loads(line) for line in f_out if line.strip()]
                # ì „ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ JSON ë¬¸ìì—´ë¡œ ë§Œë“¦
                output_string = json.dumps(output_lines_data, ensure_ascii=False, separators=(',', ':'))
            except Exception as e:
                print(f"Error processing output file pair for {input_file_path.name}. Error: {e}")
                continue

        # Negative Sample ì²˜ë¦¬
        else:
            no_exclusion_count += 1
            output_string = NO_EXCLUSION_MESSAGE

        # --- ğŸ’¡ 3. íŒŒì¼ í•˜ë‚˜ë‹¹ í•˜ë‚˜ì˜ ë ˆì½”ë“œ ìƒì„± ---
        # symbolì„ ìˆœíšŒí•˜ëŠ” ë£¨í”„ê°€ ì—†ì–´ì§€ê³ , íŒŒì¼ë‹¹ í•œë²ˆë§Œ ë ˆì½”ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        alpaca_record = {
            "instruction": main_instruction,
            "input": input_string,
            "output": output_string
        }
        all_records.append(alpaca_record)

    try:
        with open(FINAL_DATASET_FILE, 'w', encoding='utf-8') as f_final:
            for record in all_records:
                f_final.write(json.dumps(record, ensure_ascii=False) + '\n')

        matched_records = len(all_records) - no_exclusion_count
        print(f"\nâœ… Done!")
        print(f"Processed {files_processed} input files to create {len(all_records)} records.")
        print("-" * 55)
        print(f"Successfully created '{FINAL_DATASET_FILE}'")
        print(f"  - ğŸ“ Matched Records (Positive Samples): {matched_records}")
        print(f"  - ğŸ¤·â€â™€ï¸ Unmatched Records (Negative Samples): {no_exclusion_count}")
        print("-" * 55)

    except IOError as e:
        print(f"Error writing to final dataset file '{FINAL_DATASET_FILE}'. Error: {e}")


if __name__ == "__main__":
    create_alpaca_dataset()