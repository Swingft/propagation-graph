import json
import shutil
import re
from pathlib import Path
from collections import defaultdict
import multiprocessing
from tqdm import tqdm
from typing import Dict, Any

# --- ê²½ë¡œ ìƒìˆ˜ ---
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent

VALIDATED_DATA_ROOT = PROJECT_ROOT / 'llm_training_data_validated'
SPLIT_DATA_ROOT = PROJECT_ROOT / 'llm_training_data_split'
SPLIT_INPUT_DIR = SPLIT_DATA_ROOT / 'inputs'
SPLIT_OUTPUT_DIR = SPLIT_DATA_ROOT / 'outputs'

CONTEXT_MAP = {
    'methods': ['methods', 'initializers', 'deinitializers', 'subscripts', 'variables'],
    'properties': ['properties'],
    'variables': ['variables'],
    'initializers': ['classes', 'structs', 'enums', 'protocols', 'extensions', 'initializers'],
    'deinitializers': ['classes', 'structs', 'enums', 'protocols', 'extensions', 'deinitializers'],
    'subscripts': ['classes', 'structs', 'enums', 'protocols', 'extensions', 'subscripts'],
    'enumCases': ['enums', 'enumCases'],
    'classes': ['classes', 'protocols', 'structs'],
    'structs': ['structs', 'protocols', 'classes'],
    'enums': ['enums', 'protocols', 'enumCases', 'classes'],
    'protocols': ['protocols'],
    'extensions': ['extensions', 'classes', 'structs', 'enums', 'protocols'],
    'typealiases': ['typealiases', 'classes', 'structs', 'enums', 'protocols', 'extensions']
}


def setup_directories():
    """ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤."""
    print("ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
    if SPLIT_DATA_ROOT.exists(): shutil.rmtree(SPLIT_DATA_ROOT)
    SPLIT_INPUT_DIR.mkdir(parents=True, exist_ok=True)
    SPLIT_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    print("ë””ë ‰í† ë¦¬ ì¤€ë¹„ ì™„ë£Œ.")


def normalize_symbol_name(name: str) -> str:
    """ì‹¬ë³¼ ì´ë¦„ì—ì„œ íŒŒë¼ë¯¸í„° ë¶€ë¶„ì„ ì œê±°í•˜ì—¬ ì •ê·œí™”í•©ë‹ˆë‹¤. (e.g., 'myFunc(a: Int)' -> 'myFunc')"""
    return name.split('(')[0]


def parse_thinking_block(thinking_text: str) -> Dict[str, str]:
    """<thinking> ë¸”ë¡ì˜ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬, ì •ê·œí™”ëœ ì‹¬ë³¼ ì´ë¦„ì„ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¡œ ë§Œë“­ë‹ˆë‹¤."""
    # ì •ê·œì‹ íŒ¨í„´: "**Category `SymbolName`**:"ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë¸”ë¡ì„ ì°¾ìŒ
    pattern = re.compile(r"(\*\*.+?`(.+?)`\*\*:.+?)(?=\n\n\*\*|\Z)", re.DOTALL)
    matches = pattern.finditer(thinking_text)
    reasoning_map = {}
    for match in matches:
        full_block = match.group(1).strip()
        symbol_name = match.group(2).strip()
        normalized_name = normalize_symbol_name(symbol_name)
        reasoning_map[normalized_name] = full_block
    return reasoning_map


def split_single_file(file_path: Path):
    """í•˜ë‚˜ì˜ ê²€ì¦ëœ íŒŒì¼ì„ CONTEXT_MAP ê·œì¹™ì— ë”°ë¼ ì—¬ëŸ¬ ê°œì˜ ì‘ì€ íŒŒì¼ë¡œ ë¶„í• í•˜ê³ , ìƒì„±ëœ íŒŒì¼ ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # [ìˆ˜ì •] ìƒì„±ëœ íŒŒì¼ ìˆ˜ë¥¼ ì„¸ê¸° ìœ„í•œ ì¹´ìš´í„°
        created_files_count = 0

        instruction = data.get("instruction")
        original_input = data.get("input", {})
        full_output = data.get("output", {})

        original_symbols = original_input.get("symbol_data_for_analysis", {})
        json_output = full_output.get("json_output", {})

        base_name = file_path.stem.replace('validated_', '')
        relative_parent = file_path.relative_to(VALIDATED_DATA_ROOT).parent

        full_thinking_content = full_output.get("thinking", "")
        reasoning_map = parse_thinking_block(full_thinking_content)

        for group_name, source_categories in CONTEXT_MAP.items():
            grouped_input_symbols = {}
            for category in source_categories:
                if category in original_symbols:
                    grouped_input_symbols[category] = original_symbols[category]

            if not grouped_input_symbols:
                continue

            new_input_obj = original_input.copy()
            new_input_obj['symbol_data_for_analysis'] = grouped_input_symbols

            current_group_symbol_names = {
                normalize_symbol_name(symbol['symbol_name'])
                for category in grouped_input_symbols.values()
                for symbol in category
            }

            filtered_thinking_parts = [
                block for name, block in reasoning_map.items() if name in current_group_symbol_names
            ]
            filtered_thinking = "\n\n".join(filtered_thinking_parts)

            grouped_output_symbols = {}
            if group_name in json_output and json_output.get(group_name):
                grouped_output_symbols = {group_name: json_output[group_name]}

            is_positive_sample = bool(grouped_output_symbols)
            is_high_confidence_negative = (not is_positive_sample) and any(
                cat in original_symbols for cat in source_categories)

            if not (is_positive_sample or is_high_confidence_negative):
                continue

            group_dir_name = f"{group_name}_group"

            input_save_dir = SPLIT_INPUT_DIR / relative_parent / group_dir_name
            input_save_dir.mkdir(parents=True, exist_ok=True)
            input_filename = f"input_{base_name}_{group_dir_name}.json"

            final_input_record = {
                "instruction": instruction,
                "input": new_input_obj,
                "output": ""
            }
            with open(input_save_dir / input_filename, 'w', encoding='utf-8') as f:
                json.dump(final_input_record, f, indent=2, ensure_ascii=False)

            # [ìˆ˜ì •] input íŒŒì¼ ìƒì„± ì‹œ ì¹´ìš´íŠ¸ ì¦ê°€
            created_files_count += 1

            if is_positive_sample:
                output_save_dir = SPLIT_OUTPUT_DIR / relative_parent / group_dir_name
                output_save_dir.mkdir(parents=True, exist_ok=True)
                output_filename = f"output_{base_name}_{group_dir_name}.json"

                final_output_record = {
                    "thinking": filtered_thinking,
                    "json_output": grouped_output_symbols
                }
                with open(output_save_dir / output_filename, 'w', encoding='utf-8') as f:
                    json.dump(final_output_record, f, indent=2, ensure_ascii=False)

                # [ìˆ˜ì •] output íŒŒì¼ ìƒì„± ì‹œ ì¹´ìš´íŠ¸ ì¦ê°€
                created_files_count += 1

        # [ìˆ˜ì •] ì„±ê³µ ì‹œ ìƒì„±ëœ íŒŒì¼ ìˆ˜ë¥¼ ë°˜í™˜
        return created_files_count
    except Exception as e:
        return f"ì˜¤ë¥˜: {file_path.name} ì²˜ë¦¬ ì¤‘ - {e}"


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    setup_directories()

    validated_files = sorted(list(VALIDATED_DATA_ROOT.rglob("*.json")))
    if not validated_files:
        print("ë¶„í• í•  ê²€ì¦ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\nğŸš€ 2ë‹¨ê³„: ì´ {len(validated_files)}ê°œì˜ ê²€ì¦ëœ íŒŒì¼ì„ ë¶„í• í•©ë‹ˆë‹¤...")

    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:
        results = list(
            tqdm(pool.imap_unordered(split_single_file, validated_files), total=len(validated_files), desc="íŒŒì¼ ë¶„í•  ì¤‘"))

    # [ìˆ˜ì •] ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì´ ìƒì„± íŒŒì¼ ìˆ˜ì™€ ì˜¤ë¥˜ë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤.
    errors = []
    total_split_files_created = 0
    for res in results:
        if isinstance(res, int):
            total_split_files_created += res
        elif isinstance(res, str):
            errors.append(res)

    print("\nğŸ‰ 2ë‹¨ê³„ ì™„ë£Œ!")
    if errors:
        print(f"   - {len(errors)}ê°œì˜ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        for err in errors[:5]:
            print(f"     - {err}")

    print(f"   - ëª¨ë“  íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")
    # [ìˆ˜ì •] ìµœì¢… ìƒì„±ëœ íŒŒì¼ ìˆ˜ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    print(f"   - ì´ {total_split_files_created}ê°œì˜ ë¶„í• ëœ íŒŒì¼(inputs/outputs)ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == '__main__':
    main()

