import os
import json
import warnings
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


MODEL_PATH = "./out/Phi-3.5-mini-instruct_with"

if torch.cuda.is_available():
    device = "cuda"
    print(f"âœ… NVIDIA GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
else:
    device = "cpu"
    print("âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€, CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤. (ë§¤ìš° ëŠë¦¼)")

DTYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float32
print(f"ì‚¬ìš© ì •ë°€ë„(Dtype): {DTYPE}")

os.environ.setdefault("TRANSFORMERS_NO_ADVISORY_WARNINGS", "1")
warnings.filterwarnings("ignore", category=UserWarning)


NUM_TESTS = 1
TEST_DATA_PATH = "./input.jsonl"
lines_to_test = []

try:
    with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:
        lines_to_test = [f.readline() for _ in range(NUM_TESTS)]
        lines_to_test = [line for line in lines_to_test if line]

    if not lines_to_test:
        print(f"âŒ ì˜¤ë¥˜: '{TEST_DATA_PATH}' íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        exit()
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(lines_to_test)}ê°œ ë¼ì¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")

except FileNotFoundError:
    print(f"âŒ ì˜¤ë¥˜: '{TEST_DATA_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()


def run_inference(test_lines):
    print(f"ğŸš€ ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤... (ê²½ë¡œ: {MODEL_PATH})")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=DTYPE,
            trust_remote_code=True,
        )
        model.to(device)

    except OSError:
        print(f"âŒ ì˜¤ë¥˜: '{MODEL_PATH}' ê²½ë¡œì—ì„œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    model.eval()
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    for i, line in enumerate(test_lines):
        print("\n" + "=" * 25 + f" [í…ŒìŠ¤íŠ¸ {i + 1}/{len(test_lines)}] " + "=" * 25)

        data = json.loads(line)
        instruction = data.get("instruction", "")
        input_text = data.get("input", "")

        prompt = f"### Instruction:\n{instruction}\n\n"
        if input_text:
            prompt += f"### Input:\n{input_text}\n\n"
        prompt += "### Response:"

        print("â–¶ï¸  ëª¨ë¸ì— ì…ë ¥ë  ìµœì¢… í”„ë¡¬í”„íŠ¸ (ì¼ë¶€):")
        print(prompt[:500] + "...")

        inputs = tokenizer(prompt, return_tensors="pt", return_attention_mask=True).to(device)

        with torch.no_grad():
            print("\nğŸ’¬ ëª¨ë¸ ì‘ë‹µì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")
            outputs = model.generate(
                **inputs,
                use_cache=False,
                max_new_tokens=8192,
                temperature=0.1,
                top_p=0.9,
                do_sample=True,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id,
            )

        response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)

        print("\n" + "-" * 20 + " ëª¨ë¸ ìƒì„± ê²°ê³¼ " + "-" * 20)
        print(response)
        print("-" * 55)


if __name__ == "__main__":
    if lines_to_test:
        run_inference(lines_to_test)